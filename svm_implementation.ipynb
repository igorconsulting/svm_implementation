{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# important main libraries\n",
    "import cvxopt\n",
    "import numpy as np\n",
    "from utils.utils import create_dataset, plot_contour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kernel class\n",
    "\n",
    "class Kernel:\n",
    "    \"\"\"\n",
    "    A class to encapsulate various kernel functions for the SVM.\n",
    "\n",
    "    The class provides static methods for linear, polynomial, and Gaussian (RBF) kernels.\n",
    "    These methods are used to compute the similarity between data points in the feature space,\n",
    "    potentially transforming them into a higher-dimensional space for non-linear classification.\n",
    "\n",
    "    Methods:\n",
    "    - linear(x, z): Computes the linear kernel between two vectors.\n",
    "    - polynomial(x, z, p=5): Computes the polynomial kernel between two vectors with degree p.\n",
    "    - gaussian(x, z, sigma=0.1): Computes the Gaussian (RBF) kernel between two vectors with width sigma.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def linear(x, z):\n",
    "        \"\"\"Computes the linear kernel between two vectors.\"\"\"\n",
    "        return np.dot(x, z.T)\n",
    "\n",
    "    @staticmethod\n",
    "    def polynomial(x, z, p=5):\n",
    "        \"\"\"Computes the polynomial kernel between two vectors with degree p.\"\"\"\n",
    "        return (1 + np.dot(x, z.T)) ** p\n",
    "\n",
    "    @staticmethod\n",
    "    def gaussian(x, z,\n",
    "                 sigma=0.1,\n",
    "                 ):\n",
    "        \"\"\"Computes the Gaussian (RBF) kernel between two vectors with width sigma.\"\"\"\n",
    "       \n",
    "        return np.exp(-np.linalg.norm(x - z, axis=1) ** 2 / (2 * (sigma ** 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm class\n",
    "class SVM():\n",
    "    \"\"\"\n",
    "    Support Vector Machine (SVM) class for binary classification.\n",
    "\n",
    "    This implementation of SVM supports different kernel functions, including linear,\n",
    "    polynomial, and Gaussian (RBF), and solves the convex optimization problem using\n",
    "    the cvxopt library.\n",
    "\n",
    "    Parameters:\n",
    "    - kernel (function): Kernel function to use for transforming the data. Default is Gaussian.\n",
    "    - C (float): Regularization parameter. The strength of the regularization is inversely\n",
    "      proportional to C. Must be strictly positive. The penalty is a squared l2 penalty.\n",
    "\n",
    "    Attributes:\n",
    "    - X (numpy.ndarray): The input features used for training.\n",
    "    - y (numpy.ndarray): The target values used for training.\n",
    "    - K (numpy.ndarray): The kernel matrix computed from the input features.\n",
    "    - alphas (numpy.ndarray): The Lagrange multipliers for each data point in the training set.\n",
    "    - w (numpy.ndarray): The weight vector computed using the support vectors and their corresponding alphas.\n",
    "    - b (float): The bias term in the decision function.\n",
    "\n",
    "    Methods:\n",
    "    - fit(X, y): Trains the SVM model using the provided data.\n",
    "    - predict(X): Predicts the class labels for the given input features.\n",
    "    - get_parameters(alphas): Identifies the support vectors, calculates the weight vector w, and computes the bias term b.\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel = Kernel.gaussian, C=1):\n",
    "        \"\"\"\n",
    "        Initializes the SVM model with the specified kernel function and regularization parameter C.\n",
    "        \"\"\"\n",
    "        self.kernel = kernel\n",
    "        self.C = C\n",
    "        \n",
    "    def compute_kernel_matrix(self,X):\n",
    "        \"\"\"\n",
    "        Computes the kernel matrix for the input data using the specified kernel function.\n",
    "\n",
    "        Parameters:\n",
    "        - X (numpy.ndarray): Training data, a 2D array of shape (n_samples, n_features).\n",
    "\n",
    "        Returns:\n",
    "        - K (numpy.ndarray): The kernel matrix, a 2D array of shape (n_samples, n_samples).\n",
    "        \"\"\"\n",
    "        m, n = X.shape\n",
    "        m = X.shape[0]  # Number of samples\n",
    "        \n",
    "        K = np.zeros((m, m))  # Initialize the kernel matrix with zeros\n",
    "                    \n",
    "        # Calculate Kernel\n",
    "        self.K = np.zeros((m, m))\n",
    "        for i in range(m):\n",
    "            self.K[i, :] = self.kernel(X[i, np.newaxis], self.X)\n",
    "\n",
    "        return K\n",
    "        \n",
    "    def solve_svm_qp_problem(self, K,X,y):\n",
    "        \"\"\"\n",
    "        Solves the quadratic programming (QP) problem at the heart of the training process for a Support Vector Machine (SVM).\n",
    "\n",
    "        The QP problem is formulated to determine the SVM's decision function that best separates the classes in the dataset.\n",
    "\n",
    "        The standard form of the optimization problem for the softmax margin SVM can be expressed as:\n",
    "\n",
    "        Maximize with respect to alpha:\n",
    "            sum(alpha_i) - 1/2 * alpha^T * H * alpha\n",
    "        Subject to:\n",
    "            0 <= alpha_i <= C\n",
    "            sum(alpha_i * y^(i)) = 0\n",
    "\n",
    "        Which can also be written as:\n",
    "\n",
    "        Minimize with respect to alpha:\n",
    "            1/2 * alpha^T * H * alpha - 1^T * alpha\n",
    "        Subject to:\n",
    "            -alpha_i <= 0\n",
    "            alpha_i <= C\n",
    "            y^T * alpha = 0\n",
    "\n",
    "        The objective function to minimize is:\n",
    "            1/2 * alpha^T * P * alpha - q^T * alpha\n",
    "\n",
    "        Subject to constraints:\n",
    "            G * alpha <= h\n",
    "            A * alpha = b\n",
    "\n",
    "        where P is a matrix derived from the kernel function applied to the training data, q is a vector where each element is -1, \n",
    "        G and h enforce the box constraints on the Lagrange multipliers alpha_i ensuring 0 <= alpha_i <= C, and A and b ensure \n",
    "        the sum of the product of Lagrange multipliers and their corresponding labels is zero, satisfying the KKT conditions for optimality.\n",
    "        \"\"\"\n",
    "        m, n = X.shape\n",
    "        m = X.shape[0]  # Number of samples\n",
    "        P = cvxopt.matrix(np.outer(y,y) * self.K)\n",
    "        q = cvxopt.matrix(-np.ones((m,1)))\n",
    "        G = cvxopt.matrix(np.vstack((np.eye(m)*-1,np.eye(m))))\n",
    "        h = cvxopt.matrix(np.hstack((np.zeros(m), np.ones(m) * self.C)))\n",
    "        A = cvxopt.matrix(y,(1,m),'d')\n",
    "        b = cvxopt.matrix(np.zeros(1))\n",
    "        \n",
    "        cvxopt.solvers.options['show_progress'] = False\n",
    "        \n",
    "        solver = cvxopt.solvers.qp(P, q, G, h, A, b)\n",
    "        return solver\n",
    "        \n",
    "    def fit(self, X,y):\n",
    "        \"\"\"\n",
    "        Fits the SVM model to the training data.\n",
    "\n",
    "        Parameters:\n",
    "        - X (numpy.ndarray): Training data, a 2D array of shape (n_samples, n_features).\n",
    "        - y (numpy.ndarray): Target values, a 1D array of shape (n_samples,).\n",
    "\n",
    "        The method sets up the quadratic programming problem for the dual form of the SVM optimization,\n",
    "        solves it using cvxopt, and determines the Lagrange multipliers (alphas). It then calculates the\n",
    "        kernel matrix K using the chosen kernel function.\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "        # Calculate the Kernel matrix using the compute_kernel_matrix method\n",
    "        self.K = self.compute_kernel_matrix(X)\n",
    "            \n",
    "        # Solve the QP problem using the kernel matrix K and target values y\n",
    "        sol = self.solve_svm_qp_problem(self.K,X, y)\n",
    "        self.alphas = np.array(sol['x'])\n",
    "        \n",
    "    def predict(self,X):\n",
    "        \"\"\"\n",
    "        Predicts the class labels for the provided input features.\n",
    "\n",
    "        Parameters:\n",
    "        - X (numpy.ndarray): Input features, a 2D array of shape (n_samples, n_features).\n",
    "\n",
    "        Returns:\n",
    "        - y_predict (numpy.ndarray): Predicted class labels, a 1D array of shape (n_samples,).\n",
    "        The prediction is made based on the sign of the decision function: w \\cdot x + b.\n",
    "        \"\"\"\n",
    "\n",
    "        y_predict = np.zeros((X.shape[0]))\n",
    "        sv = self.get_parameters(self.alphas)\n",
    "        \n",
    "        for i in range(X.shape[0]):\n",
    "            y_predict[i] = np.sum(\n",
    "                self.alphas[sv] * self.y[sv,np.newaxis] *\n",
    "                self.kernel(X[i], self.X[sv])[:, np.newaxis]\n",
    "            )\n",
    "        \n",
    "        return np.sign(y_predict + self.b)\n",
    "    \n",
    "    def get_parameters(self, alphas):\n",
    "        \"\"\"\n",
    "        Identifies support vectors, calculates the weight vector w, and computes the bias b\n",
    "        using the support vectors and their corresponding Lagrange multipliers (alphas).\n",
    "\n",
    "        Parameters:\n",
    "        - alphas (numpy.ndarray): Lagrange multipliers for each data point in the training set.\n",
    "\n",
    "        Returns:\n",
    "        - sv (numpy.ndarray): Indices of the support vectors.\n",
    "\n",
    "        The support vectors are identified by their corresponding alphas, which are greater than a\n",
    "        small threshold but less than C. The weight vector w and bias b are then calculated using\n",
    "        these support vectors and their alphas.\n",
    "        \"\"\"\n",
    "        threshold = 1e-4\n",
    "        \n",
    "        sv = ((alphas>threshold) * (alphas <self.C)).flatten()\n",
    "        \n",
    "        self.w = np.dot(self.X[sv].T, alphas[sv]*self.y[sv, np.newaxis])\n",
    "        self.b = np.mean(self.y[sv,np.newaxis] - \n",
    "                         self.alphas[sv]*self.y[sv,np.newaxis]*self.K[sv,sv][:,np.newaxis])\n",
    "        \n",
    "        return sv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "isolatedenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
